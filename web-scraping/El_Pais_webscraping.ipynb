{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "El_Pais_webscraping",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Sz8txTtRYtGz"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import random\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_from_page(url):\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "\n",
        "  title = soup.find(\"h1\", class_ = \"a_t\").get_text()\n",
        "  date = soup.find(\"a\", id = \"article_date_p\").attrs['data-date'][:10]\n",
        "  summary = soup.find(\"h2\", class_ = \"a_st\").get_text()\n",
        "  content = \"\".join([elt.text for elt in soup.find_all(\"p\")])[:-62]\n",
        "\n",
        "  return [title, date, url, summary, content]"
      ],
      "metadata": {
        "id": "jhPiG6jVYz1u"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_topic_pages(page):\n",
        "  soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "  urls_1 = list(set([elt.find(\"a\")[\"href\"] for elt in soup.find_all(\"article\", class_='c c-d c--m ')]))\n",
        "  urls_2 = list(set([elt.find(\"a\")[\"href\"] for elt in soup.find_all(\"article\", class_='c c-o c-d c--m ')]))\n",
        "  urls_incomplete = urls_1 + urls_2\n",
        "  urls = [('https://english.elpais.com' + urls_incomplete[i]) for i in range(len(urls_incomplete))]\n",
        "  return(urls)"
      ],
      "metadata": {
        "id": "O-sNcQnOY8dV"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape(page_no, topic):\n",
        "  URL = \"https://english.elpais.com/\" + topic + \"/\" + str(page_no)\n",
        "  url_list = get_topic_pages(requests.get(URL))\n",
        "  sleep(random.uniform(2, 3))\n",
        "\n",
        "  data = []\n",
        "\n",
        "  for url in url_list:\n",
        "    data.append(scrape_from_page(url))\n",
        "    sleep(random.uniform(2, 3))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "1zwgV_ehZGDl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_loop(start_pg, end_pg, topic):\n",
        "    data = []\n",
        "    for pg in range(start_pg, end_pg):\n",
        "        data += scrape(pg, topic)\n",
        "        \n",
        "    return data"
      ],
      "metadata": {
        "id": "jhOlWjVGZPzN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = scrape_loop(1, 2, \"sports\")\n",
        "print(dt)"
      ],
      "metadata": {
        "id": "Wqlj3vQ3ZT4y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}