{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "wQB7IKO7jV7I",
    "outputId": "c1da0f1d-772e-4512-8afd-47fa0c960377"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "DC9Q4ZKtmkEU"
   },
   "outputs": [],
   "source": [
    "def scrape_from_page(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    title = soup.find(\"header\", {\"class\" : \"article-title\"}).find(\"h1\").get_text()\n",
    "    content = \"\".join([elt.get_text() for elt in soup.find(\"div\", {\"id\" : \"article_content\"}).find_all(\"p\")])\n",
    "    summary = soup.find(\"h2\", {\"class\" : \"subline\"}).get_text()\n",
    "    date = soup.time.attrs['datetime'][:10]\n",
    "\n",
    "    return [title, date, url, summary, content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0e3J1lE7qGGH"
   },
   "outputs": [],
   "source": [
    "def get_topic_pages(page):\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    urls = list(set([elt.find(\"a\")[\"href\"] for elt in soup.find_all(\"article\")]))\n",
    "    return list(filter(lambda k: 'https' in k, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2yMRJKKZu5A7"
   },
   "outputs": [],
   "source": [
    "def scrape(page_no, topic):\n",
    "    URL = \"https://www.ibtimes.co.in/\" + topic + \"page/\" + str(page_no)\n",
    "    url_list = get_topic_pages(requests.get(URL))\n",
    "    sleep(1)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for url in url_list:\n",
    "        data.append(scrape_from_page(url))\n",
    "        sleep(1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_loop(start_pg, end_pg, topic):\n",
    "    data = []\n",
    "    for pg in range(start_pg, end_pg):\n",
    "        data += scrape(pg, topic)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mEP9pMravyiJ"
   },
   "outputs": [],
   "source": [
    "dt = scrape_loop(1, 20, \"sports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"news_db.sqlite\"\n",
    "table_name = \"articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_P3tCn-WxrSE"
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "create = f'CREATE TABLE {table_name} (title VARCHAR, date DATE, url VARCHAR, summary VARCHAR, content VARCHAR)'\n",
    "delete = f'DROP TABLE {table_name}'\n",
    "\n",
    "def run(conn, query):\n",
    "    conn.cursor().execute(query)\n",
    "    conn.commit()\n",
    "    \n",
    "def table_exists(conn, table_name):\n",
    "    return conn.cursor().execute(\n",
    "  f\"\"\"SELECT name FROM sqlite_master WHERE type='table'\n",
    "  AND name='{table_name}'; \"\"\").fetchall() != []\n",
    "\n",
    "def show_table(conn, table_name):\n",
    "    return conn.execute(f\"SELECT * FROM {table_name}\").fetchall()\n",
    "    \n",
    "def insert_data(conn, table_name, data):\n",
    "    insert = f\"\"\"\n",
    "    INSERT INTO {table_name}\n",
    "    (title, date, url, summary, content)\n",
    "    VALUES (?, ?, ?, ?, ?)\n",
    "    \"\"\"\n",
    "    conn.cursor().executemany(insert, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run(conn, delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (table_exists(conn, \"articles\")): run(conn, create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data(conn, \"articles\", dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table(conn, \"articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IBTimes webscraping",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
